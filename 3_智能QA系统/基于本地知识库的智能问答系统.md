## 项目及实现
---
### 1、项目开发框架
整个框架分为三部分：
- 数据源(Data Sources):包括结构化数据、非结构化数据和代码。
- 大模型应用（Application）：以大模型为逻辑引擎，生成回答。
- 用例（Use-Cases）：构建出的系统。
![alt text](..\0_images\3\image.png){width=600}

**核心实现机制**：数据处理管道（Pipeline）
具体流程分为5步：
1. Loading：文档加载器加载Documents
2. Splitting：文本分割器切分Documets，形成 文档块
3. Storage：将文档块 以embeding形式存储的向量数据库（Vector DB），形成 嵌入片
4. Retrieval：Application从存储中检索（余弦相似度）
5. Output：把问题和相似嵌入片转递给LLM，生成答案
![alt text](..\0_images\3\image2.png){width=600}

---
### 2、具体实现
![alt text](..\0_images\3\image4.png){width=600}
#### 1） Loading
使用 LangChain中的 document_loaders加载各种格式的文本文件。
（注意：可能需要pyPDF、docx2txt等库，`pip install pyPDF docx2txt`安装既可）
```python
# 1.Load 导入Document Loaders
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import TextLoader

# 加载Documents
base_dir = r".\3_智能QA系统\OneFlower" # 文档的存放目录
documents = []
for file in os.listdir(base_dir): 
    # 构建完整的文件路径
    file_path = os.path.join(base_dir, file)
    if file.endswith('.pdf'):
        loader = PyPDFLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.docx'): 
        loader = Docx2txtLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.txt'):
        loader = TextLoader(file_path)
        documents.extend(loader.load())
```

#### 2）Splitting
```python
# 2.Split 将Documents切分成块以便后续进行嵌入和向量存储
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)
chunked_documents = text_splitter.split_documents(documents)
```
#### 3） Storage
（注意：doubao需要导入volcenginesdkarkruntime模块，`pip install volcengine-python-sdk`安装火山VPN SDK既可。另外，可能需要安装`pip3 install cryptography`模块）
```python
# 3.Store 将分割嵌入并存储在矢量数据库Qdrant中
from langchain_community.vectorstores import Qdrant
# from langchain.embeddings import OpenAIEmbeddings
# vectorstore = Qdrant.from_documents(
#     documents=chunked_documents, # 以分块的文档
#     embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入
#     location=":memory:",  # in-memory 存储
#     collection_name="my_documents",) # 指定collection_name
from typing import Dict, List, Any
from langchain.embeddings.base import Embeddings
from pydantic import BaseModel
from volcenginesdkarkruntime import Ark
class DoubaoEmbeddings(BaseModel, Embeddings):
    client: Ark = None
    api_key: str = ""
    model: str

    def __init__(self, **data: Any):
        super().__init__(**data)
        if self.api_key == "":
            self.api_key = os.environ["OPENAI_API_KEY"]
        self.client = Ark(
            base_url=os.environ["OPENAI_BASE_URL"],
            api_key=self.api_key
        )

    def embed_query(self, text: str) -> List[float]:
        """
        生成输入文本的 embedding.
        Args:
            texts (str): 要生成 embedding 的文本.
        Return:
            embeddings (List[float]): 输入文本的 embedding，一个浮点数值列表.
        """
        embeddings = self.client.embeddings.create(model=self.model, input=text)
        return embeddings.data[0].embedding

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.embed_query(text) for text in texts]

    class Config:
        arbitrary_types_allowed = True


vectorstore = Qdrant.from_documents(
    documents=chunked_documents,  # 以分块的文档
    embedding=DoubaoEmbeddings(
        model=os.environ["EMBEDDING_MODELEND"],
    ),  # 用OpenAI的Embedding Model做嵌入
    location=":memory:",  # in-memory 存储
    collection_name="my_documents",
)  # 指定collection_name
```
#### 4）Retrieval
```python
# 4. Retrieval 准备模型和Retrieval链
import logging  # 导入Logging工具
from langchain_openai import ChatOpenAI  # ChatOpenAI模型
from langchain.retrievers.multi_query import (
    MultiQueryRetriever,
)  # MultiQueryRetriever工具
from langchain.chains import RetrievalQA  # RetrievalQA链

# 设置Logging
logging.basicConfig()
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

# 实例化一个大模型工具 - OpenAI的GPT-3.5
llm = ChatOpenAI(model=os.environ["LLM_MODELEND"], temperature=0)

# 实例化一个MultiQueryRetriever
retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(), llm=llm
)

# 实例化一个RetrievalQA链
qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever_from_llm)
```
#### 5）Output： 
```python
# 5. Output 问答系统的UI实现
from flask import Flask, request, render_template

app = Flask(__name__)  # Flask APP


@app.route("/", methods=["GET", "POST"])
def home():
    if request.method == "POST":
        # 接收用户输入作为问题
        question = request.form.get("question")

        # RetrievalQA链 - 读入问题，生成答案
        result = qa_chain({"query": question})

        # 把大模型的回答结果返回网页进行渲染
        return render_template("index.html", result=result)

    return render_template("index.html")
```

### 3、运行效果
![alt text](..\0_images\3\image3.png){width=600}